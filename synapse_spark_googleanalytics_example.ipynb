{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "#Read all JSON files in your data lake.\r\n",
        "df = spark.read.load('abfss://datalakefs@mydatalake.dfs.core.windows.net/GoogleAnalytics/raw/*', format='json', multiLine=True)\r\n",
        "\r\n",
        "#Provide the schema of the JSON files\r\n",
        "df.printSchema()\r\n",
        "\r\n",
        "#Shows number of files loaded based upon the count.\r\n",
        "df.count()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Display entries in the dataframe\r\n",
        "display(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gather high level data, some of which is still metadata but scrubbed out the row entries and column mappings as we do that later.\r\n",
        "df_single = df.select(\r\n",
        "    \"containsSampledData\",\r\n",
        "    \"id\",\r\n",
        "    \"itemsPerPage\",\r\n",
        "    \"kind\",\r\n",
        "    \"nextLink\",\r\n",
        "    \"profileInfo.*\",\r\n",
        "    \"query.*\",\r\n",
        "    \"sampleSize\",\r\n",
        "    \"sampleSpace\",\r\n",
        "    \"selfLink\",\r\n",
        "    \"totalResults\",\r\n",
        "    \"totalsForAllResults.*\"\r\n",
        ");\r\n",
        "\r\n",
        "display(df_single);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#'Flatten' out nested arrays in the rows column and map the names of the columns based upon the columnHeaders column data\r\n",
        "#This example helped demonstrate the column mappings from the columnHeaders in the JSON file to the rows column\r\n",
        "#https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/753971180331031/2636709537891264/8469274372247319/latest.html\r\n",
        "\r\n",
        "column_names = map(lambda x: x['name'], df.select(\"columnHeaders\").collect()[0][0])\r\n",
        "rows = df.select(\"rows\").collect()\r\n",
        "\r\n",
        "row_list= []\r\n",
        "for row in rows:\r\n",
        "  for item in row[0]:\r\n",
        "    row_list.append(item)\r\n",
        "\r\n",
        "rows_rdd = sc.parallelize(row_list)\r\n",
        "df_rows = spark.createDataFrame(rows_rdd, list(column_names), .01)\r\n",
        "df_rows.count()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_rows)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write to datalake as Parquet in YYYY/mm/dd/HHMMSS directory structure with name of parquest as either the summary data or individual\r\n",
        "\r\n",
        "import datetime\r\n",
        "\r\n",
        "date = datetime.datetime.now()\r\n",
        "\r\n",
        "df_rows.write.parquet(\"abfss://datalakefs@mydatalake.dfs.core.windows.net/GoogleAnalytics/transformed/%s/%s/%s/%s_%s\" % (date.strftime(\"%Y\"), date.strftime(\"%m\"), date.strftime(\"%d\"), date.strftime(\"%H%M%S\"), \"ga_sessions\"))\r\n",
        "df_rows.write.parquet(\"abfss://datalakefs@mydatalake.dfs.core.windows.net/GoogleAnalytics/transformed/%s/%s/%s/%s_%s\" % (date.strftime(\"%Y\"), date.strftime(\"%m\"), date.strftime(\"%d\"), date.strftime(\"%H%M%S\"), \"ga_sessions_summary\"))\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}